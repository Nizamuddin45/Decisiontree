{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question1:**  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        " -  A Decision Tree is a supervised machine learning algorithm used for both classification and regression, but it‚Äôs most commonly used for classification tasks. A Decision Tree is like a flowchart-like structure where:\n",
        "\n",
        " - Each internal node represents a test on a feature (e.g., ‚ÄúAge > 30?‚Äù).\n",
        "\n",
        " - Each branch represents an outcome of the test (Yes/No).\n",
        "\n",
        " - Each leaf node represents a final class label (e.g., ‚ÄúApproved‚Äù or ‚ÄúRejected‚Äù).\n",
        "\n",
        " - It basically splits the dataset into smaller and smaller groups based on conditions, until the groups are as pure (similar) as possible.\n",
        "\n",
        " **How is works ?**\n",
        "\n",
        " **1.** Select the best feature to split the data ‚Äî using criteria like:\n",
        "\n",
        " - Gini Index\n",
        "\n",
        " - Entropy / Information Gain\n",
        "\n",
        "**2.** Split the dataset based on that feature‚Äôs values.\n",
        "\n",
        "**3.** Repeat the process recursively for each subset.\n",
        "\n",
        "**4.** Stop when:\n",
        "\n",
        " - All samples in a node belong to one class, or\n",
        "\n",
        " - No further improvement can be made.\n",
        "\n",
        " ---\n"
      ],
      "metadata": {
        "id": "wK_CGx2-Varv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question2:** Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "-- **Gini Impurity**\n",
        "\n",
        " - Measures how impure a node is.\n",
        "\n",
        " - Formula: Gini=1 ‚àí ‚àë(pi)2\n",
        "\n",
        " - Gini = 0 ‚Üí pure node, Gini = 0.5 ‚Üí mixed classes.\n",
        "\n",
        " - Lower Gini means better split.\n",
        "\n",
        "--**Entropy**\n",
        "\n",
        " - Measures uncertainty in a node.\n",
        "\n",
        " - Formula: ùê∏ùëõùë°ùëüùëúùëùùë¶= ‚àí‚àë( ùëùùëñlog2ùëùùëñ )\n",
        "\n",
        " - Entropy = 0 ‚Üí pure, Entropy = 1 ‚Üí most impure.\n",
        "\n",
        " - Split chosen gives highest information gain (reduction in entropy).\n",
        "\n",
        " --**Impact on Split**\n",
        "\n",
        "  - Decision Tree checks all features.\n",
        "\n",
        " - Chooses split with lowest impurity (using Gini or Entropy).\n",
        "\n",
        " - Gini ‚Üí faster, default in sklearn.\n",
        "\n",
        " - Entropy ‚Üí uses info gain concept.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "8JF8DvlIWn3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question3:** What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        " -  | Feature          | **Pre-Pruning (Early Stopping)**                                             | **Post-Pruning (Reduced Error Pruning)**           |\n",
        "| ---------------- | ---------------------------------------------------------------------------- | -------------------------------------------------- |\n",
        "| **When applied** | During tree building                                                         | After the full tree is built                       |\n",
        "| **How it works** | Stops growing the tree early using conditions (e.g., max depth, min samples) | Grows full tree, then removes unnecessary branches |\n",
        "| **Goal**         | Prevent overfitting early                                                    | Simplify the complex tree                          |\n",
        "| **Computation**  | Faster (less training time)                                                  | Slower (needs full tree first)                     |\n",
        "| **Advantage**    | Saves time and avoids overfitting                                            | Gives simpler and more accurate final model        |\n",
        "\n",
        "\n",
        "**Pre-Pruning Advantage:** Saves time and prevents overfitting early.\n",
        "**Post-Pruning Advantage:** Produces a simpler and more accurate model after checking performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Cr4Ts9O-X9F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question4:** What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        " -  Information Gain is a measure used in Decision Trees to determine which feature provides the most useful information for classifying data.It is based on the concept of Entropy, which measures the level of impurity or disorder in a dataset.\n",
        " -  When a dataset is split based on a feature, the Information Gain tells us how much entropy decreases as a result of that split ‚Äî in other words, how much more organized or pure the data becomes. A higher Information Gain means that the feature helps to make the data more homogeneous (pure), and thus, it is considered a better feature for splitting.\n",
        " - So, in Decision Trees, at each node, the algorithm selects the feature with the highest Information Gain to make the best possible split and build an effective model.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "f6DK_kNsYl66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question5:** What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        " --  **Real-World Applications**\n",
        "\n",
        " - **Banking:** Loan approval or credit risk prediction.\n",
        "\n",
        " - **Healthcare:** Disease diagnosis based on symptoms.\n",
        "\n",
        " - **Marketing:** Predicting customer churn or purchase behavior.\n",
        "\n",
        " - **Finance:** Fraud detection.\n",
        "\n",
        " - **Education:** Predicting student performance.\n",
        "\n",
        " -- **Adavantages**\n",
        "\n",
        "  - Easy to understand and visualize.\n",
        "\n",
        " - Works with both numerical and categorical data.\n",
        "\n",
        " - No need for feature scaling.\n",
        "\n",
        " - Can handle non-linear relationships.\n",
        "\n",
        " -- **Limitations**\n",
        "\n",
        "  - Prone to overfitting if not pruned.\n",
        "\n",
        " - Small data changes can change the whole tree.\n",
        "\n",
        " - Biased towards features with more categories.\n",
        "\n",
        " - Less accurate compared to ensemble methods (like Random Forest).\n",
        "\n",
        " ---\n"
      ],
      "metadata": {
        "id": "9rYwowOwZJW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question6:**Write a Python program to:\n",
        "\n",
        " - Load the Iris Dataset\n",
        "\n",
        " - Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        " -  Print the model‚Äôs accuracy and feature importances\n",
        "\n"
      ],
      "metadata": {
        "id": "8GmbrRL9aHJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnpEzK0MaiCZ",
        "outputId": "9bb06bcf-ae02-40c7-d3a0-e37f169764a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.000\n",
            "sepal width (cm): 0.019\n",
            "petal length (cm): 0.893\n",
            "petal width (cm): 0.088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question7:** Write a Python program to:\n",
        " - Load the Iris Dataset\n",
        " -  Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "NcsQUzssawtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth = 3\n",
        "model_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model_limited.fit(X_train, y_train)\n",
        "y_pred_limited = model_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully-grown Decision Tree (no depth limit)\n",
        "model_full = DecisionTreeClassifier(random_state=42)\n",
        "model_full.fit(X_train, y_train)\n",
        "y_pred_full = model_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy (max_depth=3):\", acc_limited)\n",
        "print(\"Accuracy (fully-grown tree):\", acc_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHag7Opha9nd",
        "outputId": "aa90af58-534d-4acf-9ccf-1d4317561d00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 1.0\n",
            "Accuracy (fully-grown tree): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question8:** Write a Python program to:\n",
        "\n",
        " - Load the Boston Housing Dataset\n",
        " - Train a Decision Tree Regressor\n",
        " - Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "owjMuP1Ka_Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset (replacement for Boston)\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(housing.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1ajEu89b_sY",
        "outputId": "0b4c0f5d-fcae-4a29-a18f-a373a7815efb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.523\n",
            "HouseAge: 0.052\n",
            "AveRooms: 0.049\n",
            "AveBedrms: 0.025\n",
            "Population: 0.032\n",
            "AveOccup: 0.139\n",
            "Latitude: 0.090\n",
            "Longitude: 0.089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question9:** Write a Python program to:\n",
        " - Load the Iris Dataset\n",
        " - Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        " - Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "uSb9m3LGcDcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict using best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB1N7ZsocN0F",
        "outputId": "93a8e16d-a0d1-497f-974d-454ce3a61e42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 6}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question10:**magine you‚Äôre working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        " -  Handle the missing values\n",
        " -  Encode the categorical features\n",
        " - Train a Decision Tree model\n",
        " - Tune its hyperparameters\n",
        " - Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        " -- **1. Handle Missing Values**\n",
        "\n",
        " - Check which features have missing data.\n",
        "\n",
        " - For numerical columns, fill missing values with median or mean.\n",
        "\n",
        " - For categorical columns, fill with most frequent value or a new category like \"Unknown\".\n",
        "\n",
        " - Use SimpleImputer from sklearn to handle this automatically.\n",
        "\n",
        " -- **2. Encode Categorical Features**\n",
        "\n",
        " - Convert categorical data into numbers so the model can understand.\n",
        "\n",
        " - Use One-Hot Encoding for nominal features and Ordinal Encoding if categories have an order.\n",
        "\n",
        " - This can be done easily with ColumnTransformer or OneHotEncoder.\n",
        "\n",
        " -- **3. Train the Decision Tree**\n",
        "\n",
        " - Split the dataset into training and testing sets using train_test_split().\n",
        "\n",
        " - Train a DecisionTreeClassifier (e.g., criterion='gini' or 'entropy').\n",
        "\n",
        " - Fit the model on training data and test on unseen data to check basic accuracy.\n",
        "\n",
        " -- **4. Tune Hyperparameters**\n",
        "\n",
        " - Use GridSearchCV or RandomizedSearchCV to find best values for parameters like:\n",
        "\n",
        "   - max_depth\n",
        "\n",
        "   - min_samples_split\n",
        "\n",
        "   - min_samples_leaf\n",
        "\n",
        " - This helps to prevent overfitting and improve model generalization.\n",
        "\n",
        " -- **5. Evaluate Model Performance**\n",
        "\n",
        " - Use metrics like accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        " - For medical predictions, recall (sensitivity) is often most important ‚Äî we don‚Äôt want to miss patients who actually have the disease.\n",
        "\n",
        " -- **6. Business Value**\n",
        "\n",
        " - Helps in early disease detection, improving patient outcomes.\n",
        "\n",
        " - Supports doctors in making faster and data-driven decisions.\n",
        "\n",
        " - Saves time and healthcare costs by identifying high-risk patients early.\n",
        "\n",
        " - A well-tuned Decision Tree is interpretable, so medical professionals can trust and understand the model‚Äôs reasoning.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "kn6I6wwQcTt6"
      }
    }
  ]
}